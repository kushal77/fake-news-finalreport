{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ef196e-3a45-4981-bfdd-8bc6b2221821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "file_path = \"C:/project/news_cleaned_2018_02_13.csv\"\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "\n",
    "selected_columns = [\"id\", \"domain\", \"type\", \"url\", \"content\", \"title\"]\n",
    "sample_chunks = []\n",
    "for chunk in pd.read_csv(file_path, usecols=selected_columns, chunksize=100000, on_bad_lines='warn', engine='python'):\n",
    "    sample = chunk.sample(frac=0.1, random_state=42)\n",
    "    sample_chunks.append(sample)\n",
    "    df = pd.concat(sample_chunks, ignore_index=True)\n",
    "\n",
    "train_dataframe, test_dataframe = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataframe.to_csv(\"C:/project/train_data.csv\", chunksize=100000)\n",
    "test_dataframe.to_csv(\"C:/project/test_data.csv\", chunksize=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d538b-f887-4434-88ea-8855eb4daa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import csv\n",
    "import ast\n",
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenizer(text):\n",
    "    # contractions will remove any additional english contractions such as \"'s\", \"n't\"\n",
    "    # this only works in english chars, verifying if there is non english chars\n",
    "    if text.isascii():\n",
    "        text = contractions.fix(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# features to check from data\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "# should match formats such as \"%m %d,%Y\", \"%m %d\" and numeric dates in format YYYY-MM-DD, DD/MM/YYYY, MM-DD-YYY\n",
    "date_pattern = re.compile(r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2}(?:, \\d{4})?\\b | \\b\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4}\\b | \\b\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}\\b')\n",
    "# should match decimals and integers\n",
    "numbers_pattern = re.compile(r'\\b\\d+(\\.\\d+)?\\b')\n",
    "url_count = 0\n",
    "date_count = 0\n",
    "numbers_count = 0\n",
    "\n",
    "def count_features(text):\n",
    "    urls = len(url_pattern.findall(text))\n",
    "    dates = len(date_pattern.findall(text))\n",
    "    nums = len(numbers_pattern.findall(text))\n",
    "    return urls, dates, nums\n",
    "\n",
    "\n",
    "train_data_path = 'C:/project/train_data.csv'\n",
    "tokens_output_file_path = 'C:/project/train_data_tokens.csv'\n",
    "\n",
    "chunks = []\n",
    "with open(tokens_output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"tokens\"])\n",
    "    for chunk in pd.read_csv(train_data_path , chunksize=10000, on_bad_lines='warn', engine='python'):\n",
    "        # remove NaN values from training data to apply tokenization\n",
    "        chunk['content'] = chunk['content'].astype(str).fillna('')\n",
    "        chunk_tokens = (chunk['content'].apply(tokenizer))\n",
    "        writer.writerows([tokens] for tokens in chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "        for content in chunk['content']:\n",
    "            urls, dates, nums = count_features(content)\n",
    "            url_count += urls\n",
    "            date_count += dates\n",
    "            numbers_count += nums\n",
    "\n",
    "# train_data\n",
    "train_data  = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "# check how divided the data is\n",
    "print(train_data['type'].value_counts())\n",
    "# show features in text\n",
    "print(f\"Total URLs: {url_count}\")\n",
    "print(f\"Total Dates: {date_count}\")\n",
    "print(f\"Total Numbers: {numbers_count}\")\n",
    "\n",
    "# get most common tokens from the tokenized file\n",
    "stop_words = stopwords.words('english')\n",
    "punct = [char for char in string.punctuation]\n",
    "# remove additional punctuation chars\n",
    "extra_punct = ['’', '“', '”', '—', '–', '‘', '...', '…', '``', \"''\", '--']\n",
    "\n",
    "token_counter = Counter()\n",
    "stop_words_token_counter = Counter()\n",
    "stemmed_token_counter = Counter()\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "with open(tokens_output_file_path, newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) # skipping header 'token'\n",
    "    for row in reader:\n",
    "        # the format of each row is string and needs to be converted to list\n",
    "        tokens = ast.literal_eval(row[0])\n",
    "        tokens = [token for token in tokens if token not in punct and token not in extra_punct]\n",
    "        token_counter.update(tokens)\n",
    "        # remove stopwords and add them to separate counter\n",
    "        tokens_without_stopwords = [token for token in tokens if token.lower() not in stop_words]\n",
    "        stop_words_token_counter.update(tokens_without_stopwords)\n",
    "        # apply stemming\n",
    "        stemmed_token_counter.update(stemmer.stem(token) for token in tokens_without_stopwords)\n",
    "\n",
    "most_common_tokens = token_counter.most_common(100)\n",
    "print(f\"100 most common tokens: {most_common_tokens}\")\n",
    "most_common_tokens_without_stopwords = stop_words_token_counter.most_common(100)\n",
    "print(f\"100 most common tokens without stopwords: {most_common_tokens_without_stopwords}\")\n",
    "most_common_tokens_after_stemming = stemmed_token_counter.most_common(100)\n",
    "print(f\"100 most common tokens after stemming: {most_common_tokens_after_stemming}\")\n",
    "\n",
    "original_vocabulary_size = token_counter.total()\n",
    "stop_words_vocabulary_size = stop_words_token_counter.total()\n",
    "stemmed_vocabulary_size = stemmed_token_counter.total()\n",
    "\n",
    "# Compute reduction rates\n",
    "stopwords_reduction_rate = ((original_vocabulary_size- stop_words_vocabulary_size) / original_vocabulary_size) * 100\n",
    "stemming_reduction_rate = ((stop_words_vocabulary_size - stemmed_vocabulary_size) / stop_words_vocabulary_size) * 100\n",
    "print(f\"Original vocabulary size: {original_vocabulary_size}\")\n",
    "print(f\"Vocabulary size after removing stopwords: {stop_words_vocabulary_size}\")\n",
    "print(f\"Reduction rate after removing stopwords: {stopwords_reduction_rate}\")\n",
    "print(f\"Vocabulary size after stemming: {stemmed_vocabulary_size}\")\n",
    "print(f\"Reduction rate after stemming: {stemming_reduction_rate}\")\n",
    "\n",
    "# visualize most common words\n",
    "words, counts = zip(*most_common_tokens_without_stopwords)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 100 Most Frequent Words After Stopwords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2556dde-403e-45f8-b3c6-8700e5dfcf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Results:\n",
      "Accuracy  : 0.9460\n",
      "Precision : 0.9901\n",
      "Recall    : 0.8889\n",
      "F1 Score  : 0.9368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "\n",
    "# Load train data\n",
    "train_file_path = \"C:/project/train_data.csv\"\n",
    "selected_columns = [\"type\", \"content\"]\n",
    "\n",
    "train_data_chunks = []\n",
    "for chunk in pd.read_csv(train_file_path, usecols=selected_columns, chunksize=100000, on_bad_lines='warn', engine='python'):\n",
    "    chunk = chunk.dropna()\n",
    "    train_data_chunks.append(chunk)\n",
    "train_data = pd.concat(train_data_chunks, ignore_index=True).sample(n=1000, random_state=42)\n",
    "\n",
    "# Load test data\n",
    "test_file_path = \"C:/project/train_data.csv\"\n",
    "test_data_chunks = []\n",
    "for chunk in pd.read_csv(test_file_path, usecols=selected_columns, chunksize=100000, on_bad_lines='warn', engine='python'):\n",
    "    chunk = chunk.dropna()\n",
    "    test_data_chunks.append(chunk)\n",
    "test_data = pd.concat(test_data_chunks, ignore_index=True).sample(n=1000, random_state=42)\n",
    "\n",
    "\n",
    "fake_labels = {'fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable'}\n",
    "not_fake_labels = {'political', 'reliable'}\n",
    "\n",
    "train_data['binary_label'] = train_data['type'].apply(lambda x: 1 if x in not_fake_labels else 0)\n",
    "test_data['binary_label'] = test_data['type'].apply(lambda x: 1 if x in not_fake_labels else 0)\n",
    "\n",
    "# Features and labels\n",
    "X_train = train_data['content']\n",
    "y_train = train_data['binary_label']\n",
    "X_test = test_data['content']\n",
    "y_test = test_data['binary_label']\n",
    "\n",
    "# TF-IDF Vectorization \n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Naive Bayes model for classification\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_vec, y_train)\n",
    "y_pred = nb_model.predict(X_test_vec)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(\"Naive Bayes Classification Results:\")\n",
    "print(f\"Accuracy  : {accuracy:.4f}\")\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall    : {recall:.4f}\")\n",
    "print(f\"F1 Score  : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348ae97-66b7-469b-b81b-88caa49f43d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
